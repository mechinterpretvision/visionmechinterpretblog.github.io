<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='Mitigating Compositional Issues in Text-to-Image Generative Models via Enhanced Text Embeddings. arXiv 2024.'/>
    <meta property='og:url' content='https://github.com/samyadeepbasu/LocoGen'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>On Mechanistic Knowledge Localization in Text-to-Image Generative Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Mechanistic Interpretability Beyond Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://samyadeepbasu.github.io">Samyadeep Basu</a><sup></sup><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://k1rezaei.github.io/">Keivan Rezaei</a><sup></sup><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://priyathamkat.com">Sriram Balasubramaniam</a><sup>1</sup>,
            </span>
               <span class="author-block">
              <a href="https://priyathamkat.com">Arman Zarei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://priyathamkat.com">Soheil Feizi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1 </sup>University of Maryland</span>&nbsp; &nbsp; 
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.13730"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>T2I-CausalTrace</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://proceedings.mlr.press/v235/basu24b.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>T2I-CrossAttn</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.01583"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>ViT-Internal</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.07844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Compositionality</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.04236"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>MLLM-Internal</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In the recent times, a plethora of mechanistic interpretability methods have been developed, used towards understanding and modifying different aspects of language models.
            However, progress towards understanding multimodal and vision models have been slow when compared to the interest in mechanistic interpretability for language models.
            In the past one year, Feizi Lab has been developing and adapting interpretability tools to understand the inner workings of multimodal models and vision models,
            with a downstream emphasis on applications (e.g., removing copyrighted content in T2I models, improving compositionality, mitigating spurious correlations in vision models, updating rare knowledge in MLLMs).
            This blog provides a comprehensive overview of our year-long efforts in mechanistic interpretability beyond language models and discusses the open research problems in this area. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><span style="font-variant: small-caps;">Causal Tracing</span> For Text-to-Image Generative Models</h2>

        <!-- Prompt Interpolation image -->

        <!--
        <div class="content has-text-centered">
            <img src="./static/images/main_figure.jpg" width="90%">
        </div>
        <div class="content has-text-justified">
          <p>
            To address the <i>universal</i> knowledge localization framework absence across different text-to-image models,
            we introduce the concept of <i>>mechanistic localization</i> that aims to identify a small number of layers which control the generation of distinct visual attributes,
            across a spectrum of text-to-image models. 
            To achieve this, we propose <span style="font-variant: small-caps;">LocoGen</span>, a method that finds a subset of cross-attention layers in the UNet such that when the input to their key and value matrices is changed,
            output generation for a given visual attribute (e.g., "style") is modified (see above Figure).
            This intervention in the intermediate layers has a direct effect on the output -- therefore <span style="font-variant: small-caps;">LocoGen</span>
            measures the direct effect of intermediate layers, as opposed to indirect effects in causal tracing. 
          </p>
          <br>
        </div> -->

          
        <!-- Prompt Interpolation image -->
        
        <!-- <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a yellow book and a red vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red apple and a green train.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a green leaf and a yellow butterfly.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/A black and white cat sitting in a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue backpack and a red book.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/A bathroom with green tile and a red shower curtain.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue bench and a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue backpack and a red chair.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue bowl and a red train.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a brown book and a red sheep.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a fluffy towel and a glass cup.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a plastic container and a fluffy teddy bear.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red chair and a gold clock.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red pen and a blue notebook.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a round cookie and a square container.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a wooden floor and a fluffy rug.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/The leather jacket and fluffy scarf keep the cold at bay.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/wooden pencil and a glass plate.png"
              class="interpolation-image"/>
            </div>
          </div> -->

        <!-- </div> -->
        <!--
        <h3 class="title is-4"><span style="font-variant: small-caps;">LocoGen</span> effectivley identifies important layers!</h3>
        <div class="content has-text-centered">
          <img src="./static/images/locogen_localization/locogen.jpg" width="100%">
        </div>
        
        <div class="content has-text-justified">
          <p>
            Images generated by intervening on the layers identified by <span style="font-variant: small-caps;">LocoGen</span>
            across various open-source text-to-image models.
            We compare the original generation vs. generation by intervening on the layers identified with <span style="font-variant: small-caps;">LocoGen</span>
            along with a target prompt.
            We find that across various text-to-image models, visual attributes such as <i>style, objects, facts</i> can be manipulated by intervening only on a very small fraction of cross-attention layers.
          </p>
          <br>
        </div>
        <br>

        <h3 class="title is-4">How does <span style="font-variant: small-caps;">LocoGen</span> find these layers?</h3>
        <div class="content has-text-centered">
          <img src="./static/images/locogen_localization/algorithm.png" width="60%">
        </div>
        
        <div class="content has-text-justified">
          <p>
            To apply <span style="font-variant: small-caps;">LocoGen</span> for a particular attribute,
            we obtain a set of input prompts T that include the particular attribute and corresponding set of prompts T'
            where each prompt T'<sub>i</sub> there is analogous to a corresponding prompt T<sub>i</sub> in T except that the particular attribute is removed/updated.
            These prompts serve to create altered images and assess the presence of the specified attribute within them.
            Let c<sub>i</sub> be the text-embedding of T<sub>i</sub> and c'<sub>i</sub> be that of T'<sub>i</sub>.
            Given m (number of layers to consider) and M (number of total layers),
            we examine all M-m+1 possible candidates for controlling layers.
            For each of them, we generate N altered images where i-th image is generated by giving c'<sub>i</sub> as the input embedding to selected $m$ layers and c<sub>i</sub> to other ones.
            Then we measure the CLIP-Score of original text prompt T<sub>i</sub> to the generated image for <i>style, objects</i> and target text prompt T'<sub>i</sub>$ to the generated image for <i>facts</i>.
            For <i>style</i> and <i>objects</i>, drop in CLIP-Score shows the removal of the attribute while for <i>facts</i> increase in score shows similarity to the updated fact.
            We take the average of the mentioned score across all 1 &leq; i &leq; N.
            By doing that for all candidates,
            we report the one with minimum average CLIP-Score for <i>style, objects</i> and maximum average CLIP-Score for <i>facts</i>.
            These layers could be candidate layers controlling the generation of the specific attribute.
            Above Algorithm provides the pseudocode to find the best candidate.
          </p>
          <br>
        </div>
          -->
        <br>


        <!-- Prompt Interpolation image -->
        <!-- <h3 class="title is-4">(ii) <tt>WiCLP</tt> improves Cross-Attention Masks!</h3>
        
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a blue backpack and a red bench.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a green bench and a yellow dog.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a red book and a yellow vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a bathroom has brown wall and gold counters.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a black cat sitting in a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a green blanket and a blue pillow.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div> -->

        <!-- <div class="content has-text-justified">
          <p>
            [TODO] :-?
          </p>
          <br>
        </div> -->
        <!-- <br><br> -->

        <!-- <h3 class="title is-4">(iii) <span style="font-variant: small-caps;">Switch-Off</span> enables <tt>WiCLP</tt> to preserve Model Utility!</h3>
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a yellow boat and a blue dog.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red book and a yellow vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A bathroom has brown wall and gold counters.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue bear and a brown boat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A bathroom with green tile and a red shower curtain.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red backpack and a blue book.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue cup and a red orange.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a green blanket and a blue pillow.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a metallic watch and a fluffy towel.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a white car and a red sheep.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A red bathroom has a white towel on the bar.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a plastic bag and a leather chair.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a green leaf and a yellow butterfly.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a pink elephant and a brown giraffe.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red cup and a blue suitcase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue backpack and a red chair.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Fine-tuning models or adding modules to a base model often results in a degradation of image quality and an increase in the Fréchet Inception Distance (FID) score.
            To balance the trade-off between improved compositionality and the quality of generated images for clean prompts --an important issue in existing work-- Inspired by Hertz et al <sup><a href="https://arxiv.org/abs/2208.01626" target="_blank">[1]</a></sup>,
            we adopt <span style="font-variant: small-caps;">Switch-Off</span>, where we apply the linear projection only during the initial steps of inference.
            More precisely, given a time-step threshold &tau;, for <i>t </i> &geq; &tau;,
            we use <tt>WiCLP</tt>, while for for <i>t </i> < &tau;,
            we use the unchanged embedding (output of CLIP text encoder) as the input to the cross-attention layers.
            As seen above, &tau; = 800 provides a correct compositional scene while preserving the quality.
          </p>
          <br>
        </div> -->
      </div>
    </div>


    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><span style="font-variant: small-caps;">Localizing Knowledge</span> in Cross-Attention Layers</h2>
        <br>
        Given the limitations of causal tracing for localizing knowledge within text encoders of recent models,
        we sought to develop a more generalizable approach for knowledge localization in text-to-image models—one that could be effectively scaled and applied to modern architectures.
        In this work, we investiage whether knowledge representing artistic styles, objects, or facts can be localized in cross-attention layers of text-to-image models.
        Interestingly, we observed that among significant number of cross attention layers, only a select few play a key role in controlling the generation of specific concepts.
        In fact, the model heavily relies on information provided by those layers to generate those concepts.
        e.g., we observed that in Stable Diffusion XL, layers 45-48 among X layers are responsible for Van Gogh style.
        Modifying the input only in these specific layers leads the U-Net to produce an image that noticeably loses its Van Gogh characteristics.
        Localization within few layers enables an efficient and surgical model editiing method that aims to minimally modify cross-attention layers in those specified layers.
        This happens by carefully editing key value matrices within those layers, mapping original text prompts to representations that exclude the targeted concept.
        Importantly, the edited models retain their general utility and continue to generate high-quality images when prompted with general inputs.

        Webpage <a href="https://t2i-knowledge-localization.github.io">(Link)</a>

        <!-- Prompt Interpolation image -->
        <!-- <h3 class="title is-4">Source (i): Errorneous Attention Contributions in CLIP Text Encoder</h3> -->

        <!--
        <div class="content has-text-centered">
            <img src="./static/images/locoedit/locoedit.jpg" width="100%">
        </div>
        <div class="content has-text-justified">
        <p>
          <span style="font-variant: small-caps;">LocoGen</span> extracts the set of cross-attention layers that control specific visual attributes (e.g., style).
          We refer to this set as C_<sub>loc</sub>, a subset of cross-attention layers from which the knowledge about these attributes is influenced.
          Each layer contains value and key matrices, and the goal is to modify these matrices in a way that transforms the attention mechanism input
          from an original prompt (such as “A house in the style of Van Gogh”) to a target prompt (such as “A house in the style of a painting”).
          This adjustment affects how the visual attribute appears in the generated image.

          </p>
        </div> -->

        <!-- Prompt Interpolation image -->
        <!--
        <div class="content has-text-centered">
            <img src="./static/images/locoedit/formula.png" width="50%">
        </div>
        <div class="content has-text-justified">
          <p>
            In fact, we solve the above optimization problem where
            <ul>
              <li>X<sub>orig</sub> refers to the text-embeddings of original prompts -- having the artistic style, copyright object, or outdated knowledge;</li>
              <li>X<sub>target</sub> refers to text-embeddings of target prompts -- including updated knowledge or lacking the visual attribute;</li>
              <li>W<sub>l</sub><sup>k</sup> that refers to key matrix in localized layer l;</li>
              <li>
              <span style="position: relative;">W<span style="position: absolute; top: -10px; left: 3px">^</span></span>
              <sub>l</sub><sup>k</sup> that refers to key matrix in localized layer l.</li>              
            </ul>
          </p>
          <br>
        </div> -->

          
      </div>
    </div>
  
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3 has-text-centered">Mechanistically Understanding Internal Model Components Across Different ViTs</h2>
      <!--
      <div class="content has-text-centered">
          <img src="./static/images/zero_shot/zero_shot.jpg" width="50%">
      </div>
      <div class="content has-text-justified">
        <p>
        As seen in above Figure,
        neuron-level modification at inference time is effective at removing styles.
        This shows that knowledge about a particular style can be even more localized to a few neurons.
        It is noteworthy that the extent of style removal increases with the modification of more neurons, albeit with a trade-off in the quality of generated images.
        This arises because modified neurons may encapsulate information related to other visual attributes.
        </p>
      </div> -->

        
    </div>
  </div>

<div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3 has-text-centered">Compositionality in Diffusion Models</h2>
      <!--
      <div class="content has-text-centered">
          <img src="./static/images/zero_shot/zero_shot.jpg" width="50%">
      </div>
      <div class="content has-text-justified">
        <p>
        As seen in above Figure,
        neuron-level modification at inference time is effective at removing styles.
        This shows that knowledge about a particular style can be even more localized to a few neurons.
        It is noteworthy that the extent of style removal increases with the modification of more neurons, albeit with a trade-off in the quality of generated images.
        This arises because modified neurons may encapsulate information related to other visual attributes.
        </p>
      </div> -->
    </div>
  </div>

<div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3 has-text-centered">Understanding Internal Components in Multimodal Language Models</h2>
      <!--
      <div class="content has-text-centered">
          <img src="./static/images/zero_shot/zero_shot.jpg" width="50%">
      </div>
      <div class="content has-text-justified">
        <p>
        As seen in above Figure,
        neuron-level modification at inference time is effective at removing styles.
        This shows that knowledge about a particular style can be even more localized to a few neurons.
        It is noteworthy that the extent of style removal increases with the modification of more neurons, albeit with a trade-off in the quality of generated images.
        This arises because modified neurons may encapsulate information related to other visual attributes.
        </p>
      </div> -->
    </div>
  </div>

        
</div>
  
    <!--/ Matting. -->
    <!-- <div class="container is-max-desktop">
    
      Latent space editing applications
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Related Works</h2>
          <div class="content has-text-justified">
            <p>
              <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
            </p>
          </div>
          Prompt Interpolation image
        </div>
      </div>

    Concurrent Work.
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO’s Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), an Amazon Research Award and an award from Capital One.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>
@inproceedings{basu2024mechanistic,
  title={On Mechanistic Knowledge Localization in Text-to-Image Generative Models},
  author={Basu, Samyadeep and Rezaei, Keivan and Kattakinda, Priyatham and Morariu, Vlad I and Zhao, Nanxuan and Rossi, Ryan A and Manjunatha, Varun and Feizi, Soheil},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
</code></pre>
  </div>
</section>



<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/corgi_input.jpeg", "label": "Generated by SDXL-Diffusion2GAN (512px)",},
  {"src": "./static/images/corgi_output.jpeg", "label": "8 x Upsampled by GigaGAN (4K)",}
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Generated by Diffusion2GAN (512px)",
  };
  let outputImage = {
    label: "8 x Upsampled by GigaGAN (4K)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpeg")
  outputImage.src = "./static/images/".concat(name, "_output.jpeg")

  let images = [inputImage, outputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
