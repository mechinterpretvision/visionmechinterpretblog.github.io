<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='Mechanistic Interpretability Beyond Language Models'/>
    <!-- <meta property='og:url' content='https://github.com/samyadeepbasu/LocoGen'/> -->
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Mechanistic Interpretability Beyond Language Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Mechanistic Interpretability Beyond Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://samyadeepbasu.github.io">Samyadeep Basu</a>,
            </span>
            <span class="author-block">
              <a href="https://k1rezaei.github.io/">Keivan Rezaei</a>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://sriram.live/">Sriram Balasubramanian</a>,
            </span>
               <span class="author-block">
              <a href="https://priyathamkat.com">Arman Zarei</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.umd.edu/~sfeizi">Soheil Feizi</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1 </sup>University of Maryland</span>&nbsp; &nbsp; 
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.13730"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>T2I-CausalTrace</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://proceedings.mlr.press/v235/basu24b.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>T2I-CrossAttn</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#vit-decompose"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-globe"></i>
                  </span>
                  <span>Decompose ViTs</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.07844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Compositionality</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.04236"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>MLLM-Internal</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In the recent times, a plethora of mechanistic interpretability methods have been developed, used towards understanding and modifying different aspects of language models.
            However, progress towards understanding multimodal and vision models have been slow when compared to the interest in mechanistic interpretability for language models.
            In the past one year, Feizi Lab has been developing and adapting interpretability tools to understand the inner workings of multimodal models and vision models,
            with a downstream emphasis on applications (e.g., removing copyrighted content in T2I models, improving compositionality, mitigating spurious correlations in vision models, updating rare knowledge in MLLMs).
            This blog provides a comprehensive overview of our year-long efforts in mechanistic interpretability beyond language models and discusses the open research problems in this area. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><span style="font-variant: small-caps;">Causal Tracing</span> For Text-to-Image Generative Models</h2>

        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a yellow boat and a blue dog.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red book and a yellow vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A bathroom has brown wall and gold counters.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue bear and a brown boat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A bathroom with green tile and a red shower curtain.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red backpack and a blue book.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue cup and a red orange.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a green blanket and a blue pillow.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a metallic watch and a fluffy towel.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a white car and a red sheep.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A red bathroom has a white towel on the bar.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a plastic bag and a leather chair.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a green leaf and a yellow butterfly.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a pink elephant and a brown giraffe.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red cup and a blue suitcase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue backpack and a red chair.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Fine-tuning models or adding modules to a base model often results in a degradation of image quality and an increase in the Fréchet Inception Distance (FID) score.
            To balance the trade-off between improved compositionality and the quality of generated images for clean prompts --an important issue in existing work-- Inspired by Hertz et al <sup><a href="https://arxiv.org/abs/2208.01626" target="_blank">[1]</a></sup>,
            we adopt <span style="font-variant: small-caps;">Switch-Off</span>, where we apply the linear projection only during the initial steps of inference.
            More precisely, given a time-step threshold &tau;, for <i>t </i> &geq; &tau;,
            we use <tt>WiCLP</tt>, while for for <i>t </i> < &tau;,
            we use the unchanged embedding (output of CLIP text encoder) as the input to the cross-attention layers.
            As seen above, &tau; = 800 provides a correct compositional scene while preserving the quality.
          </p>
          <br>
        </div> -->
      </div>
    </div>


    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><span style="font-variant: small-caps;">Localizing Knowledge</span> in Cross-Attention Layers</h2>
        <br>
        Given the limitations of causal tracing for localizing knowledge within text encoders of recent models,
        we sought to develop a more generalizable approach for knowledge localization in text-to-image models—one that could be effectively scaled and applied to modern architectures.
        In this work, we investiage whether knowledge representing artistic styles, objects, or facts can be localized in cross-attention layers of text-to-image models.
        Interestingly, we observed that among significant number of cross attention layers, only a select few play a key role in controlling the generation of specific concepts.
        In fact, the model heavily relies on information provided by those layers to generate those concepts.
        e.g., we observed that in Stable Diffusion XL, layers 45-48 among X layers are responsible for Van Gogh style.
        Modifying the input only in these specific layers leads the U-Net to produce an image that noticeably loses its Van Gogh characteristics.
        Localization within few layers enables an efficient and surgical model editiing method that aims to minimally modify cross-attention layers in those specified layers.
        This happens by carefully editing key value matrices within those layers, mapping original text prompts to representations that exclude the targeted concept.
        Importantly, the edited models retain their general utility and continue to generate high-quality images when prompted with general inputs.

        Webpage <a href="https://t2i-knowledge-localization.github.io">(Link)</a>

        <!-- Prompt Interpolation image -->
        <!-- <h3 class="title is-4">Source (i): Errorneous Attention Contributions in CLIP Text Encoder</h3> -->

        <!--
        <div class="content has-text-centered">
            <img src="./static/images/locoedit/locoedit.jpg" width="100%">
        </div>
        <div class="content has-text-justified">
        <p>
          <span style="font-variant: small-caps;">LocoGen</span> extracts the set of cross-attention layers that control specific visual attributes (e.g., style).
          We refer to this set as C_<sub>loc</sub>, a subset of cross-attention layers from which the knowledge about these attributes is influenced.
          Each layer contains value and key matrices, and the goal is to modify these matrices in a way that transforms the attention mechanism input
          from an original prompt (such as “A house in the style of Van Gogh”) to a target prompt (such as “A house in the style of a painting”).
          This adjustment affects how the visual attribute appears in the generated image.

          </p>
        </div> -->

        <!-- Prompt Interpolation image -->
        <!--
        <div class="content has-text-centered">
            <img src="./static/images/locoedit/formula.png" width="50%">
        </div>
        <div class="content has-text-justified">
          <p>
            In fact, we solve the above optimization problem where
            <ul>
              <li>X<sub>orig</sub> refers to the text-embeddings of original prompts -- having the artistic style, copyright object, or outdated knowledge;</li>
              <li>X<sub>target</sub> refers to text-embeddings of target prompts -- including updated knowledge or lacking the visual attribute;</li>
              <li>W<sub>l</sub><sup>k</sup> that refers to key matrix in localized layer l;</li>
              <li>
              <span style="position: relative;">W<span style="position: absolute; top: -10px; left: 3px">^</span></span>
              <sub>l</sub><sup>k</sup> that refers to key matrix in localized layer l.</li>              
            </ul>
          </p>
          <br>
        </div> -->

          
      </div>
    </div>
  
  <div id="vit-decompose" class="columns is-centered">
    <div class="column is-full-width">
      <a href="https://sriram.live/vit-decompose/">
      <h2 class="title is-3 link has-text-centered">Mechanistically Understanding Model Components Across Different ViTs</h2>
      </a>
      <br>
      <div class="content has-text-justified">

        <p>
          In text-to-image models, we were able to analyze and control the generation of concepts via causal tracing by perturbing carefully chosen text tokens in the input which correspond to the concept of interest. However, in vision models with image inputs, it is more challenging to carefully control and perturb the chosen concepts, which means that causal tracing is less effective in this setting. Thus, rather than understanding the model from the input side, we aim to understand the model from the <i>representation side</i>. Specifically, we use the following approach:
        </p>  

        <ol>
          <li> We <i>decompose</i> the final representation as a sum of contributions from different model <i>components</i>. Furthermore, each of these component contributions can be decomposed over the image patch <i>tokens</i> </li>
          <li> We then <i>map</i> each of these contributions to CLIP space where they can be interpreted using CLIP's <i>shared image-text representation space</i></li>
          <li> Using the CLIP text encoder, we identify which components are responsible for encoding which concepts, and use them to retrieve or segment images, or ablate them to mitigate spurious correlations</li>

        </ol>

        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/vit_decompose/img_retrieval_1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/vit_decompose/token_viz.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/vit_decompose/img_retrieval_2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/vit_decompose/table.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>

        <p>
          Thus, even though the image space is unstructured and does not allow for controlled concept perturbations, we can still localize concepts to a significant extent by understanding the model components in terms of their contribution to the structured image-text representation space indexed via CLIP. 
        </p> 
<br>

        <p>
          <b>Future directions: </b> Although we were able to localize certain concepts via this approach, there are still many open questions. For one, we found that the direct controbutions of model components are not limited to one role - they are responsible for jointly encoding many concepts along with other concepts. This makes it difficult to ablate them without affecting other concepts. One way to address this is to probably select a subspace within each component that is responsible for encoding a single concept. Another direction is to understand how these components interact with each other to encode concepts, as it is likely that concept components are built out of simpler ones.
        </p> 
       
      </div>

      

        
    </div>
  </div>

<div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3 has-text-centered">Compositionality in Diffusion Models</h2>
      <!--
      <div class="content has-text-centered">
          <img src="./static/images/zero_shot/zero_shot.jpg" width="50%">
      </div>
      <div class="content has-text-justified">
        <p>
        As seen in above Figure,
        neuron-level modification at inference time is effective at removing styles.
        This shows that knowledge about a particular style can be even more localized to a few neurons.
        It is noteworthy that the extent of style removal increases with the modification of more neurons, albeit with a trade-off in the quality of generated images.
        This arises because modified neurons may encapsulate information related to other visual attributes.
        </p>
      </div> -->
    </div>
  </div>

<div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3 has-text-centered">Understanding Internal Components in Multimodal Language Models</h2>
      <!--
      <div class="content has-text-centered">
          <img src="./static/images/zero_shot/zero_shot.jpg" width="50%">
      </div>
      <div class="content has-text-justified">
        <p>
        As seen in above Figure,
        neuron-level modification at inference time is effective at removing styles.
        This shows that knowledge about a particular style can be even more localized to a few neurons.
        It is noteworthy that the extent of style removal increases with the modification of more neurons, albeit with a trade-off in the quality of generated images.
        This arises because modified neurons may encapsulate information related to other visual attributes.
        </p>
      </div> -->
    </div>
  </div>

        
</div>
  


  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>
@inproceedings{basu2024mechanistic,
  title={On Mechanistic Knowledge Localization in Text-to-Image Generative Models},
  author={Basu, Samyadeep and Rezaei, Keivan and Kattakinda, Priyatham and Morariu, Vlad I and Zhao, Nanxuan and Rossi, Ryan A and Manjunatha, Varun and Feizi, Soheil},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
</code></pre>
  </div>
</section>



<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/corgi_input.jpeg", "label": "Generated by SDXL-Diffusion2GAN (512px)",},
  {"src": "./static/images/corgi_output.jpeg", "label": "8 x Upsampled by GigaGAN (4K)",}
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Generated by Diffusion2GAN (512px)",
  };
  let outputImage = {
    label: "8 x Upsampled by GigaGAN (4K)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpeg")
  outputImage.src = "./static/images/".concat(name, "_output.jpeg")

  let images = [inputImage, outputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
