<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='Mitigating Compositional Issues in Text-to-Image Generative Models via Enhanced Text Embeddings. arXiv 2024.'/>
    <meta property='og:url' content='https://github.com/samyadeepbasu/LocoGen'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>On Mechanistic Knowledge Localization in Text-to-Image Generative Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">On Mechanistic Knowledge Localization in Text-to-Image Generative Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://samyadeepbasu.github.io">Samyadeep Basu</a><sup>*</sup><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://k1rezaei.github.io/">Keivan Rezaei</a><sup>*</sup><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://priyathamkat.com">Priyatham Kattakinda</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://ryanrossi.com/">Ryan Rossi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/umd.edu/mazdamoayeri">Cherry Zhao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/vlad-morariu/">Vlad Morariu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/varun-manjunatha/">Varun Manjunatha</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.umd.edu/~sfeizi/">Soheil Feizi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1 </sup>University of Maryland</span>&nbsp; &nbsp; <span class="author-block"><sup>2 </sup>Adobe Research</span>
          </div>

          <div class="is-size-5 publication-venue">
            Proceedings of the 41st International Conference on Machine Learning<br>ICML 2024
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://proceedings.mlr.press/v235/basu24b.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/html/2405.01008v2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/samyadeepbasu/LocoGen/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Identifying layers within text-to-image models which control visual attributes can facilitate efficient model editing through closed-form updates.
            Recent work, leveraging causal tracing show that early Stable-Diffusion variants confine knowledge primarily to the first layer of the CLIP text-encoder,
            while it diffuses throughout the UNet.
            Extending this framework, we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing fails in pinpointing localized knowledge,
            highlighting challenges in model editing.
            To address this issue, we introduce the concept of <i>mechanistic localization</i> in text-to-image models,
            where knowledge about various visual attributes (e.g., "style", "objects", "facts") can be <i>mechanistically</i> localized
            to a small fraction of layers in the UNet, thus facilitating efficient model editing.
            We localize knowledge using our method <span style="font-variant: small-caps;">LocoGen</span> which measures the direct effect of intermediate layers to output generation by performing interventions in the cross-attention layers of the UNet. 
            We then employ <span style="font-variant: small-caps;">LocoEdit</span>, a fast closed-form editing method across popular open-source text-to-image models (including the latest SD-XL)
            and explore the possibilities of neuron-level model editing.
            Using <i>mechanistic localization</i>, our work offers a better view of successes and failures in localization-based text-to-image model editing.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><span style="font-variant: small-caps;">LocoGen</span>: Mechanistic Knowledge Localization</h2>

        <!-- Prompt Interpolation image -->
        
        <div class="content has-text-centered">
            <img src="./static/images/main_figure.jpg" width="90%">
        </div>
        <div class="content has-text-justified">
          <p>
            To address the <i>universal</i> knowledge localization framework absence across different text-to-image models,
            we introduce the concept of <i>>mechanistic localization</i> that aims to identify a small number of layers which control the generation of distinct visual attributes,
            across a spectrum of text-to-image models. 
            To achieve this, we propose <span style="font-variant: small-caps;">LocoGen</span>, a method that finds a subset of cross-attention layers in the UNet such that when the input to their key and value matrices is changed,
            output generation for a given visual attribute (e.g., "style") is modified (see above Figure).
            This intervention in the intermediate layers has a direct effect on the output -- therefore <span style="font-variant: small-caps;">LocoGen</span>
            measures the direct effect of intermediate layers, as opposed to indirect effects in causal tracing. 
          </p>
          <br>
        </div>
        <!-- Prompt Interpolation image -->
        
        <!-- <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a yellow book and a red vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red apple and a green train.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a green leaf and a yellow butterfly.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/A black and white cat sitting in a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue backpack and a red book.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/A bathroom with green tile and a red shower curtain.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue bench and a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue backpack and a red chair.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue bowl and a red train.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a brown book and a red sheep.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a fluffy towel and a glass cup.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a plastic container and a fluffy teddy bear.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red chair and a gold clock.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red pen and a blue notebook.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a round cookie and a square container.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a wooden floor and a fluffy rug.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/The leather jacket and fluffy scarf keep the cold at bay.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/wooden pencil and a glass plate.png"
              class="interpolation-image"/>
            </div>
          </div> -->

        <!-- </div> -->
        
        <h3 class="title is-4"><span style="font-variant: small-caps;">LocoGen</span> effectivley identifies important lazers!</h3>
        <div class="content has-text-centered">
          <img src="./static/images/locogen_localization/locogen.jpg" width="90%">
        </div>
        
        <div class="content has-text-justified">
          <p>
            Images generated by intervening on the layers identified by <span style="font-variant: small-caps;">LocoGen</span>
            across various open-source text-to-image models.
            We compare the original generation vs. generation by intervening on the layers identified with <span style="font-variant: small-caps;">LocoGen</span>
            along with a target prompt.
            We find that across various text-to-image models, visual attributes such as <i>style, objects, facts</i> can be manipulated by intervening only on a very small fraction of cross-attention layers.
          </p>
          <br>
        </div>
        <br>

        <h3 class="title is-4">How does <span style="font-variant: small-caps;">LocoGen</span> find these layers?</h3>
        <div class="content has-text-centered">
          <img src="./static/images/locogen_localization/algorithm.png" width="60%">
        </div>
        
        <div class="content has-text-justified">
          <p>
            To apply <span style="font-variant: small-caps;">LocoGen</span> for a particular attribute,
            we obtain a set of input prompts T that include the particular attribute and corresponding set of prompts T'
            where each prompt T'<sub>i</sub> there is analogous to a corresponding prompt T<sub>i</sub> in T except that the particular attribute is removed/updated.
            These prompts serve to create altered images and assess the presence of the specified attribute within them.
            Let c<sub>i</sub> be the text-embedding of T<sub>i</sub> and c'<sub>i</sub> be that of T'<sub>i</sub>.
            Given m (number of layers to consider) and M (number of total layers),
            we examine all M-m+1 possible candidates for controlling layers.
            For each of them, we generate N altered images where i-th image is generated by giving c'<sub>i</sub> as the input embedding to selected $m$ layers and c<sub>i</sub> to other ones.
            Then we measure the CLIP-Score of original text prompt T<sub>i</sub> to the generated image for <i>style, objects</i> and target text prompt T'<sub>i</sub>$ to the generated image for <i>facts</i>.
            For <i>style</i> and <i>objects</i>, drop in CLIP-Score shows the removal of the attribute while for <i>facts</i> increase in score shows similarity to the updated fact.
            We take the average of the mentioned score across all 1 &leq; i &leq; N.
            By doing that for all candidates,
            we report the one with minimum average CLIP-Score for <i>style, objects</i> and maximum average CLIP-Score for <i>facts</i>.
            These layers could be candidate layers controlling the generation of the specific attribute.
            Above Algorithm provides the pseudocode to find the best candidate.
          </p>
          <br>
        </div>
        <br>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">(ii) <tt>WiCLP</tt> improves Cross-Attention Masks!</h3>
        
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a blue backpack and a red bench.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a green bench and a yellow dog.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a red book and a yellow vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a bathroom has brown wall and gold counters.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a black cat sitting in a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a green blanket and a blue pillow.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>

        <!-- <div class="content has-text-justified">
          <p>
            [TODO] :-?
          </p>
          <br>
        </div> -->
        <br><br>

        <h3 class="title is-4">(iii) <span style="font-variant: small-caps;">Switch-Off</span> enables <tt>WiCLP</tt> to preserve Model Utility!</h3>
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a yellow boat and a blue dog.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red book and a yellow vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A bathroom has brown wall and gold counters.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue bear and a brown boat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A bathroom with green tile and a red shower curtain.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red backpack and a blue book.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue cup and a red orange.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a green blanket and a blue pillow.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a metallic watch and a fluffy towel.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a white car and a red sheep.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/A red bathroom has a white towel on the bar.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a plastic bag and a leather chair.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a green leaf and a yellow butterfly.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a pink elephant and a brown giraffe.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a red cup and a blue suitcase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/switch_off_visualizations/a blue backpack and a red chair.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Fine-tuning models or adding modules to a base model often results in a degradation of image quality and an increase in the Fréchet Inception Distance (FID) score.
            To balance the trade-off between improved compositionality and the quality of generated images for clean prompts --an important issue in existing work-- Inspired by Hertz et al <sup><a href="https://arxiv.org/abs/2208.01626" target="_blank">[1]</a></sup>,
            we adopt <span style="font-variant: small-caps;">Switch-Off</span>, where we apply the linear projection only during the initial steps of inference.
            More precisely, given a time-step threshold &tau;, for <i>t </i> &geq; &tau;,
            we use <tt>WiCLP</tt>, while for for <i>t </i> < &tau;,
            we use the unchanged embedding (output of CLIP text encoder) as the input to the cross-attention layers.
            As seen above, &tau; = 800 provides a correct compositional scene while preserving the quality.
          </p>
          <br>
        </div>
      </div>
    </div>


    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Possible Sources of Compositional Issues</h2>
        <div class="content has-text-justified">
          <p>
            We investigate two possible sources of compositionality issues in text-to-image models.
          </p>
        </div>
        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">Source (i): Errorneous Attention Contributions in CLIP Text Encoder</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/attn_cont/attn_cont.jpg" width="100%">
        </div>
        <div class="content has-text-justified">
          <p>
            Above Figure visualizes the attention contribution of both T5 and CLIP text-encoder in the last layer &ell; = 11
            for the prompt "a green bench and a red car".
            Ideally, the attention mechanism should guide the token "car" to focus more on "red" than "green",
            but in the last layer of the CLIP text-encoder, "car" significantly attends to "green".
            In contrast, T5 shows a more consistent attention pattern, with "red" contributing more to the token "car" and "green" contributing more to the token "bench".
          </p>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">Source (ii): Sub-Optimality of CLIP Text-Encoder</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/opt_text_embedding/opt_text_embedding.png" width="80%">
        </div>
        <div class="content has-text-justified">
          <p>
            we observe that the UNet is capable of generating compositional scenes if propoer text-embeddings is given as the input.
            This proper embedding is obtained by freezing the UNet and optimizing diffusion loss over images with high VQA scores,
            using the text embedding as the optimization variable.
            We consistently improve VQA scores across a variety of compositional prompts (i.e., color, texture, and shape).
            This indicates that CLIP text-encoder does not output the proper text-embedding suitable for generating compositional scenes.
            However, that optimized embedding space exists, highlighting the ability of UNet to generate coherent compositional scenes when a proper text-embedding is given.
          </p>
          <br>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4"><tt>WiCLP</tt> Achieves Superior Compositional Performance on Stable Diffusion Models in T2I-CompBench!</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/experiments/results.png" width="90%">
        </div>
        <div class="content has-text-justified">
          <p>
            VQA scores of our method and other discussed baselines are provided in above Table.
            As shown, <tt>WiCLP</tt> significantly improves upon the baselines and
            achieves higher VQA scores compared to other state-of-the-art methods, despite its simplicity.
          </p>
        </div>
      </div>
    </div>
  </div>
  
    <!--/ Matting. -->
    <!-- <div class="container is-max-desktop">
    
      Latent space editing applications
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Related Works</h2>
          <div class="content has-text-justified">
            <p>
              <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
            </p>
          </div>
          Prompt Interpolation image
        </div>
      </div>

    Concurrent Work.
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO’s Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), an Amazon Research Award and an award from Capital One.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>
      @inproceedings{basu2024mechanistic,
        title={On Mechanistic Knowledge Localization in Text-to-Image Generative Models},
        author={Basu, Samyadeep and Rezaei, Keivan and Kattakinda, Priyatham and Morariu, Vlad I and Zhao, Nanxuan and Rossi, Ryan A and Manjunatha, Varun and Feizi, Soheil},
        booktitle={Forty-first International Conference on Machine Learning},
        year={2024}
      }
}</code></pre>
  </div>
</section>



<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/corgi_input.jpeg", "label": "Generated by SDXL-Diffusion2GAN (512px)",},
  {"src": "./static/images/corgi_output.jpeg", "label": "8 x Upsampled by GigaGAN (4K)",}
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Generated by Diffusion2GAN (512px)",
  };
  let outputImage = {
    label: "8 x Upsampled by GigaGAN (4K)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpeg")
  outputImage.src = "./static/images/".concat(name, "_output.jpeg")

  let images = [inputImage, outputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
